{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitpython3virtualenv143883b3a812423ea6b9707fcfa3d3d7",
   "display_name": "Python 3.6.8 64-bit ('python3': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/work/seongjinpark/tomcat-speech/speech_recognition'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.preprocessing import generate_dict\n",
    "from helper.wav2vec_helper import load_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-da1e90b44f238bc4\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /work/seongjinpark/.cache/huggingface/datasets/csv/default-da1e90b44f238bc4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45e17766775746faa0b37fb8b6356ec2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b329e8912c44eab9b4d48dcc8f2c88d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset csv downloaded and prepared to /work/seongjinpark/.cache/huggingface/datasets/csv/default-da1e90b44f238bc4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'text'],\n        num_rows: 540\n    })\n    test: Dataset({\n        features: ['file', 'text'],\n        num_rows: 138\n    })\n})\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import FloatProgress\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# train_path = \"/data/seongjinpark/chalearn/train/audio_16\"\n",
    "# test_path = \"/data/seongjinpark/chalearn/dev/audio_16\"\n",
    "# train_text = \"/data/seongjinpark/chalearn/train/gold_and_utts.tsv\"\n",
    "# test_text = \"/data/seongjinpark/chalearn/dev/gold_and_utts.tsv\"\n",
    "audio_path = \"/data/seongjinpark/MUStARD/utterances_final_16\"\n",
    "# train_text = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/chalearn/train.tsv\"\n",
    "# test_text = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/chalearn/dev.tsv\"\n",
    "train_text = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/mustard/train.tsv\"\n",
    "test_text = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/mustard/test.tsv\"\n",
    "\n",
    "train_csv = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/mustard/train.csv\"\n",
    "test_csv = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/mustard/dev.csv\"\n",
    "\n",
    "vocab_json = \"/work/seongjinpark/tomcat-speech/speech_recognition/data/mustard/vocab.json\"\n",
    "\n",
    "train_dict = generate_dict(audio_path, train_text, train_csv, header=False)\n",
    "test_dict = generate_dict(audio_path, test_text, test_csv, header=False)\n",
    "\n",
    "chalearn_data = load_train_test(train_csv, test_csv)\n",
    "print(chalearn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HEY MY FATHER'S HOUSE DOES THAT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>YES THERE'S NOTHING QUITE LIKE THE SLIGHTLY WIDENED EYES OF MILDLY STARTLED</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>WELL THAT WADY TOOK HIGHWES PICTURES OF MY JUNK WAST NIGHT FOR CWAIGSWIST</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CRAZY BITCH</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ACTUALLY I YESTERDAY I WAS SMOKING AGAIN TODAY I AM SMOKING STILL</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>WEĀ€™RE NOT PEEKING</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>OH BOY I JUST CAN'T WATCH IT'S TOO SCARY</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>THE BLUNT INSTRUMENT THAT WILL BE THE FOCUS OF MY MURDER TRIAL</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>WELL DON'T  'CHA WANNA</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>AS YOU MAY KNOW I'VE BEEN EXPERIMENTING WITH ELEVATED ANXIETY LEVELS AND I THOUGHT WHAT BETTER WAY TO INCREASE MY DISCOMFORT THAN TO SUBJECT MYSELF TO AN EVENING OF TASTELESS UNCENSORED CROTCH TALK</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(chalearn_data[\"train\"].remove_columns([\"file\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=540.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9614242147e944a691e63f4739bece54"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=138.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73148d5109814179ad822806bd45d415"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "chars_to_ignore_regex = '[,\\?\\.\\!\\-\\;\\:\\\"<>€]'\n",
    "int_put_space = '([\\d])([A-z])'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    batch[\"text\"] = batch[\"text\"].replace(\"é\", \"e\")\n",
    "    batch[\"text\"] = batch[\"text\"].replace(\"ñ\", \"n\")\n",
    "    batch[\"text\"] = batch[\"text\"].replace(\"õ\", \"o\")\n",
    "    batch[\"text\"] = batch[\"text\"].replace(\"ā\", \"azz\")\n",
    "    batch[\"text\"] = batch[\"text\"].replace(\"™\", \"\")\n",
    "\n",
    "    return batch\n",
    "\n",
    "chalearn_data = chalearn_data.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>thank you</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>uh okay the fifth dentist caved and not they're all recommending trident</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gee if only she were one and had no idea what a birthday was</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>certainly i'd like to raise two points number one i think they iarei talking about penises and number two these mimosas are kicking my little brown ass</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>yes i'd like a sevenday course of penicillin some uh syrup of ipecac to induce vomiting and a mint</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>see joe that's why your parents told you not to jump on your bed</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>all right cut let's pick again pick again</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>wow you look just like your son mrs tribbiani</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>thatos good just keep rubbing your head that will turn back time</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>chloe switch with me there's some guys here that got a crush on you</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "show_random_elements(chalearn_data[\"train\"].remove_columns([\"file\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6e2c8d956974c0ca369038afe6a1ea5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cadb732850b4f7698b91ac40386d225"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "  all_text = \" \".join(batch[\"text\"])\n",
    "  vocab = list(set(all_text))\n",
    "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = chalearn_data.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=chalearn_data.column_names[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['h', 'a', 'o', 'v', 'n', 'p', 'x', 'f', 'y', 'm', ' ', 'q', 'b', 's', 'u', 'j', '|', 'w', 'r', 'd', 'c', \"'\", 'e', 'i', 'l', 't', 'z', 'k', 'g']]\n"
     ]
    }
   ],
   "source": [
    "print(vocabs['train']['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'h': 0,\n",
       " 'o': 1,\n",
       " 'a': 2,\n",
       " 'x': 3,\n",
       " 'v': 4,\n",
       " 'p': 5,\n",
       " 'n': 6,\n",
       " 'f': 7,\n",
       " 'y': 8,\n",
       " 'm': 9,\n",
       " ' ': 10,\n",
       " 'q': 11,\n",
       " 'b': 12,\n",
       " 's': 13,\n",
       " 'u': 14,\n",
       " 'j': 15,\n",
       " '|': 16,\n",
       " 'w': 17,\n",
       " 'r': 18,\n",
       " 'd': 19,\n",
       " 'c': 20,\n",
       " \"'\": 21,\n",
       " 'e': 22,\n",
       " 'i': 23,\n",
       " 'l': 24,\n",
       " 't': 25,\n",
       " 'z': 26,\n",
       " 'k': 27,\n",
       " 'g': 28}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "print(len(vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor.save_pretrained(\"wav2vec2-base-chalearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    "
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=135.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c542cd16b8c243d38de139b301f9a331"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=135.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12f30016bd034e24aabcf5e8f3f6d6db"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#2', max=135.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c178c0fae5c040ec8c77e98d9c9c1ffe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#3', max=135.0, style=ProgressStyle(description_width='in…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf21a0a3352a41f198f3b59be64a8aef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "    "
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=35.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91d5d308410247aba614c87dbe58e431"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=35.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3e0e322efba467eb5f644bb56cbd67f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#2', max=34.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe4be1e6a77f4d4997dac1c5855ec541"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#3', max=34.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b3bb51e4c5d4107885b820b83a1fda6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = sf.read(batch[\"file\"])\n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sampling_rate\"] = sampling_rate\n",
    "    batch[\"target_text\"] = batch[\"text\"]\n",
    "    return batch\n",
    "\n",
    "chalearn_data = chalearn_data.map(speech_file_to_array_fn, remove_columns=chalearn_data.column_names[\"train\"], num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/work/seongjinpark/python3/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "    "
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=17.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9eb7d2ba493543cebd59113942e2e3bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=17.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1383edfa9aaa430598fb13c45f290365"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#2', max=17.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c5be16e22c54b4aa1928adc079b78b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#3', max=17.0, style=ProgressStyle(description_width='ini…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "293daafacf704e30b0afa561bf0e7828"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "    "
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#0', max=5.0, style=ProgressStyle(description_width='init…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bab24b99b86c443e91cbf5b7c6c32f63"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#1', max=5.0, style=ProgressStyle(description_width='init…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29e3a105ad2c478bb35f8c0af861dcb0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#2', max=5.0, style=ProgressStyle(description_width='init…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74abf80c51574a98a47663ac9c240d20"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='#3', max=5.0, style=ProgressStyle(description_width='init…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a57d77aecaa64c10bbdd9c46d7b0f5a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "\n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "chalearn_prepared = chalearn_data.map(prepare_dataset, remove_columns=chalearn_data.column_names[\"train\"], batch_size=8, num_proc=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", \n",
    "    gradient_checkpointing=True, \n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-base-timit-demo\",\n",
    "  output_dir=\"wav2vec2-base-chalearn\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=40,\n",
    "  fp16=True,\n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=chalearn_prepared[\"train\"],\n",
    "    eval_dataset=chalearn_prepared[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/work/seongjinpark/python3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2720 : < :, Epoch 0.01/40]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/work/seongjinpark/python3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work/seongjinpark/python3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work/seongjinpark/python3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work/seongjinpark/python3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/work/seongjinpark/python3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2720, training_loss=2.4464933479533477, metrics={'train_runtime': 4277.5735, 'train_samples_per_second': 0.636, 'total_flos': 1.0836328725801907e+18, 'epoch': 40.0})"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"wav2vec2-base-chalearn/checkpoint-2500\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"wav2vec2-base-chalearn/checkpoint-2500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=138.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d3067f9ef88456c96a493ddc3408a57"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def map_to_result(batch):\n",
    "    model.to(\"cuda\")\n",
    "    input_values = processor(\n",
    "        batch[\"speech\"],\n",
    "        sampling_rate=batch[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    \n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    return batch\n",
    "\n",
    "results = chalearn_data[\"test\"].map(map_to_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test WER: 0.849\n"
     ]
    }
   ],
   "source": [
    "print(\"Test WER: {:.3f}\".format(wer_metric.compute(predictions=results[\"pred_str\"], references=results[\"target_text\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pred_str</th>\n      <th>target_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;s&gt;noh&lt;s&gt; i'&lt;s&gt; me&lt;s&gt;s&lt;s&gt;in&lt;s&gt;</td>\n      <td>amazing</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;s&gt;no&lt;s&gt; ro&lt;s&gt;se&lt;s&gt; [UNK]o&lt;s&gt;n to the c&lt;s&gt;o&lt;s&gt;rnor&lt;s&gt; and o&lt;s&gt;pen&lt;s&gt; hr s&lt;s&gt;t&lt;s&gt;a&lt;s&gt;</td>\n      <td>no rose go to the corner and open a stand</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;s&gt;h&lt;s&gt; you and&lt;s&gt;oh&lt;s&gt; s&lt;s&gt;ur&lt;s&gt;and wh&lt;s&gt;l w&lt;s&gt;er&lt;s&gt; &lt;s&gt;u&lt;s&gt;t &lt;s&gt;t whit i' we [UNK]et en&lt;s&gt;[UNK]&lt;s&gt;a&lt;s&gt;[UNK]ed to&lt;s&gt; &lt;s&gt;i&lt;s&gt;l&lt;s&gt; &lt;s&gt;a&lt;s&gt; w&lt;s&gt;e [UNK]et a itl a &lt;s&gt; h&lt;s&gt;o&lt;s&gt;use start a f&lt;s&gt;a&lt;s&gt;mil&lt;s&gt;y&lt;s&gt; &lt;s&gt; n jo&lt;s&gt;y our s&lt;s&gt;o&lt;s&gt;n&lt;s&gt;s&lt;s&gt;et&lt;s&gt; yea&lt;s&gt;r&lt;s&gt;s to&lt;s&gt;[UNK]ether&lt;s&gt;e&lt;s&gt;ve&lt;s&gt;al&lt;s&gt;d &lt;s&gt;youre yours&lt;s&gt;elf wo&lt;s&gt;m&lt;s&gt;an&lt;s&gt;</td>\n      <td>you and oh sure and while we're at it why don't we get engaged too why don't we get a little house start a family enjoy our sunset years together do you hear yourself woman</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;s&gt;yo're[UNK]onon&lt;s&gt; to te&lt;s&gt; me how you did&lt;s&gt; tha&lt;s&gt;t&lt;s&gt;</td>\n      <td>you're gonna have to tell me how you did that</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;s&gt; we [UNK]ot a bo&lt;s&gt;cke&lt;s&gt;</td>\n      <td>we got a box</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>n't ne&lt;s&gt;d a rea&lt;s&gt;s&lt;s&gt;on&lt;s&gt; it's m&lt;s&gt;y&lt;s&gt; h&lt;s&gt;ho&lt;s&gt;use&lt;s&gt;</td>\n      <td>i don't need a reason it's my house</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>&lt;s&gt; &lt;s&gt;oh&lt;s&gt; you'&lt;s&gt; ha&lt;s&gt;rdl&lt;s&gt;y outof the wo&lt;s&gt;d&lt;s&gt;s&lt;s&gt;the&lt;s&gt; you stil rube the ris&lt;s&gt; of i&lt;s&gt;f&lt;s&gt;e&lt;s&gt;ction&lt;s&gt; &lt;s&gt;a blu&lt;s&gt;d&lt;s&gt; c&lt;s&gt;lo&lt;s&gt;t&lt;s&gt; you'e po&lt;s&gt;s&lt;s&gt;ibility ha&lt;s&gt; an&lt;s&gt; &lt;s&gt;e&lt;s&gt;i&lt;s&gt;t&lt;s&gt;ent&lt;s&gt;if s&lt;s&gt;er&lt;s&gt;[UNK]&lt;s&gt;on&lt;s&gt; lhant a bua&lt;s&gt;rn&lt;s&gt; s&lt;s&gt;p&lt;s&gt;iter l&lt;s&gt;ake e&lt;s&gt;[UNK]s in your n&lt;s&gt;o&lt;s&gt;s&lt;s&gt;</td>\n      <td>oh you're hardly out of the woods no you still run the risk of infection a blood clot the possibility that an inattentive surgeon let a barn spider lay eggs in your nose</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>&lt;s&gt;e&lt;s&gt;yo &lt;s&gt;o&lt;s&gt; tha&lt;s&gt;t &lt;s&gt;a [UNK]lo&lt;s&gt;ry esti&lt;s&gt;f&lt;s&gt;in&lt;s&gt; wa&lt;s&gt;s&lt;s&gt; r&lt;s&gt;i[UNK]ht&lt;s&gt; &lt;s&gt;eventualy the ridhem &lt;s&gt;i&lt;s&gt;s&lt;s&gt; [UNK]o&lt;s&gt;in[UNK] to [UNK]et you&lt;s&gt;</td>\n      <td>yeah either that or gloria estefan was right eventually the rhythm is going to get you</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>&lt;s&gt;no&lt;s&gt; stupet pickd up thescro&lt;s&gt;p&lt;s&gt;ilo&lt;s&gt;s a&lt;s&gt;t in his &lt;s&gt;o&lt;s&gt;n&lt;s&gt;</td>\n      <td>no stuart picked out those throw pillows all on his own</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>&lt;s&gt;wh&lt;s&gt;y&lt;s&gt;caus she couldn&lt;s&gt; s&lt;s&gt;e&lt;s&gt;in[UNK] &lt;s&gt;a&lt;s&gt;nd&lt;s&gt; &lt;s&gt;pla&lt;s&gt;[UNK] a&lt;s&gt;t&lt;s&gt;a&lt;s&gt;r&lt;s&gt; &lt;s&gt;a&lt;s&gt;nd do&lt;s&gt; bo&lt;s&gt;th at the s&lt;s&gt;ame t&lt;s&gt;ime&lt;s&gt;</td>\n      <td>why because she can sing and play guitar and do both at the same time</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "show_random_elements(results.remove_columns([\"speech\", \"sampling_rate\"]))"
   ]
  }
 ]
}